---
layout:     post
title:      Tarea 3
date:       2020-08-01 12:31:19
summary:    Tarea 3 del curso Deep Learning, con videos y material
categories: Curso Deep Learning
---
# Tarea 3: Regularización y Optimización <br/> CC6204 Deep Learning, Universidad de Chile 

Original file is located at
    https://colab.research.google.com/drive/1KT7X9npOInkBzPdgwdqjA76Sbc6ieM6P
 
**Fecha de entrega: 20 de noviembre de 2020 ([Hoja de respuestas](https://colab.research.google.com/drive/1B57157HgoGygJYK1XICDP-RbIE7HHzq2))**

En esta tarea programarás distintos métodos de regularización y optimización para las redes que construiste en la Tarea 2. Además entrenarás tu red en MNIST y estimarás su desempeño usando un conjunto de prueba.

El material necesario para resolver esta tarea es el siguiente:
* [Video: Generalización, Test-Dev-Train set, e Intro. a Regularización](https://www.youtube.com/watch?v=5gAJeY-HHtg)
* [Video: Ensemble, Dropout](https://www.youtube.com/watch?v=4cJlTns7noE)
* [Video: Inicialización, Normalización y Batch Normalization](https://www.youtube.com/watch?v=izOwC2my1Kw)
* [Video: Algoritmos de Optimización](https://www.youtube.com/watch?v=FBsiDndtdVg)

IMPORTANTE: A menos que se exprese lo contrario, sólo podrás utilizar las clases y funciones en el módulo [`torch`](https://pytorch.org/docs/stable/torch.html).

(por Jorge Pérez, https://github.com/jorgeperezrojas, [@perez](https://twitter.com/perez))

# Parte 1: Regularización y Generalización

## 1a) Regularización por *weight decay*

En clases vimos un método muy popular de regularización basado en penalizar el tamaño de los parámetros (no bias) agregando un nuevo término en la función de pérdida. También vimos que una manera alternativa (y posiblemente más fácil de programar) para lograr exactamente el mismo efecto es con el concepto de *weight decay* que en cada paso del descenso de gradiente, "achica un poco" cada parámetro antes de actualizarlo. Para esto simplemente debes modificar tu clase de descenso estocástico de gradiente `SGD` de manera que el inicializador reciba un nuevo argumento `beta`. La idea es que en cada paso de actualización, antes de actualizar los parámetros, los multipliques por `(1 - beta)`, es decir la actualización de cada parámetro `p` ahora debiera ser de la forma `p = (1-beta)*p - lr*p.grad`


```python
# Tu código debiera continuar así

class SGD():
  def __init__(self, parameters, lr, beta=0):
    # lo que sea necesario inicializar
    
    pass
  
  def step():
    # actualiza acá los parámetros a partir de los gradientes
    # y considera el nuevo valor beta
    pass
```

### Observación

Hay muchas formas equivalentes de implementar la regularización por *weight decay* y distintas librerías las implementan de formas alternativas. Por ejemplo, en el caso de `pytorch` los optimizadores reciben en general un argumento llamado `weight_decay` (ver por ejemplo el código de [`torch.optim.SGD`](https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html#SGD)) y para cada parámetro, lo que hacen es sumarle al gradiente de cada parámetro `p` el valor `weight_decay*p`, antes de hacer un paso del descenso de gradiente. Esto implica que la actualización de un paso del descenso de gradiente sea de la forma `p = p - lr*(p.grad + weight_decay*p)`. No es difícil notar que esta formulación es equivalente a la anterior considerando el valor de `beta` definido como `weight_decay*lr`.

## 1b) Regularización por dropout

Implementa el método de dropout para apagar aleatoriamente ciertas neuronas de tu red. Para esto el inicializador de tu red debe recibir otro parámetro opcional que llamaremos `keep_prob` que será una lista de probabilidades que indicarán la probabilidad de mantener las neuronas de cada capa. La lista debe tener largo $L+1$ e indicará las probabilidades incluyendo la capa de input. Por ejemplo, si tu red es inicializada como

```python
red_neuronal = FFNN(300, [50,30], [relu,sig], 10, keep_prob=[1.0, 0.5, 0.7])
```

esto quiere decir que en la implementación de dropout, las neuronas de la capa de input deben mantenerse con probabilidad $$1.0$$ (o sea, nunca deben borrarse), las neuronas de la primera capa escondida deben mantenerse con probabilidad $$0.5$$ y las de la segunda capa escondida deben mantenerse con probabilidad $$0.7$$.

Debes modificar los métodos `forward` y `backward` para que ambos consideren las probabilidades de dropout. Debes agregar también un argumento Booleano opcional `predict` a la función `forward` de tu red que asegure que las probabilidades de dropout no se consideren en una pasada hacia adelante que se haga para testear la red (y no para entrenarla).

Recuerda que la mejor forma de implementar dropout es con la técnica de "inverted dropout" que lo que hace es elegir para la capa $$i$$ una máscara de bits $$M^{(i)}$$ de acuerdo a la probabilidad $$p_i$$ de mantener encendidas las neuronas de esa capa, y luego actualiza la capa escondida como
$$h^{(i)}:=h^{(i)} * \frac{M^{(i)}}{p_i}$$

Dada esta implementación, cuando se haga el forward con la opción `predict` simplemente se puede considerar que cada $$p_i$$ es $1$ (o sea, todas las neuronas están encendidas).

```python
# Tu código debiera continuar como sigue

class FFNN():
  def __init__(self, F, l_h, l_a, C, keep_prob=None):
    # debes crear los parámetros necesarios para hacer 
    # dropout en cada capa dependiendo de keep_prob
    pass
  
  def forward(x, predict=False):
    # debes modificar esta función para considerar el dropout
    # y preocuparte de no considerarlo cuando (predict=True)
    pass
  
  def backward(x,y,y_pred):
    # computar acá todos los gradientes considerando el dropout 
    pass
```

### Observación

Al igual que pare el caso de la regularización por *weight decay* hay muchas formas equivalentes de implementar la técnica de dropout y distintas librerías toman distintas decisiones. Por ejemplo, en el caso de `pytorch` cuando se considera el dropout, la probabilidad que se define es la probabilidad `p` de **apagar las neuronas** (a diferencia de nuestra decisión de definir la probabilidad de mantener prendidas las neuronas). Es evidente que ambas formulaciones son equivalentes haciendo `p = 1 - keep_prob`.

## 1c) Entrenamiento y generalización sobre MNIST 

Usa tu red neuronal para entrenar con los datos de MNIST. Debes entrenar usando el conjunto de entrenamiento (train set) y probar usando el conjunto de prueba (test set). 

En este caso nos importa sobre todo el aplicar métodos de regularización a la red para mejorar la generalización (disminuir la diferencia entre error en entrenamiento y error en prueba). Entrena y grafica cómo varía la pérdida y el acierto en el conjunto de entrenamiento y en el conjunto de prueba primero sin usar regularización, y luego añade regularización para obtener una mejor generalización. Reporta al menos dos configuraciones que usen regularización y discute acerca de cómo varía la pérdida y el acierto en el conjunto de prueba comparado con cuando no usas regularización. 

Nota: Si quieres, puedes hacer esta parte después de que implementes la incialización y optimizaciones de la siguiente parte de la tarea, pero en esta parte nos interesa sobre todo la generalización.

# Parte 2: Optimización

## 2a) Inicialización de Xavier/He

En esta parte programarás la [inicialización de Xavier](http://proceedings.mlr.press/v9/glorot10a.html) que considera la inicialización de los pesos según el tamaño de cada capa. En particular para el tensor de parámetros $$W^{(i)}$$ de dimensiones $$(d_{i-1},d_{i})$$ (o sea, el que relaciona la salida de la capa $$i-1$$ y la entrada de la capa $$i$$), la inicialización de Xavier define cada parámetro $$w$$ en $$W^{(i)}$$ como

$$w := r \sqrt{\frac{1}{d_{i-1}}}$$

donde $$r$$ es un número aleatorio con distribución normal unitaria (media 0 y varianza 1). Recuerda que para el caso en donde las funciones de activación de la capa $$i$$ sean `relu` la inicialización debiera llevar un $$2$$ (también conocida como [inicialización de He](https://arxiv.org/abs/1502.01852)) como se muestra a continuación:

$$w := r \sqrt{\frac{2}{d_{i-1}}}$$

Cambia el inicializador de la clase de tu red neuronal agregándole un argumento opcional `init` con el que puedas decidir el tipo de inicialización de parámetros que quieras. En particular, si el valor del parámetros es el string `xavier` entonces se debe implementar la inicialización descrita (y si no se especifica nada, deberías usar la inicialización como lo habías hecho hasta la tarea pasada).

### Observación

Las dos formas de inicializar parámetros son *heurísticas* y tienen diversas variantes. Una variante estándar es si considerar una distribución normal (el valor de $$r$$ arriba) o una distribución uniforme. El [artículo original de X. Glorot & Y. Bengio](http://proceedings.mlr.press/v9/glorot10a.html) usa una distribución uniforme mientras el [artículo de K. He *et al.*](https://arxiv.org/abs/1502.01852) usa una normal. En el caso de `pytorch` se usa una [distribución uniforme](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/linear.py#L86). Si quieres puedes usar el mismo argumento `init` del inicializador para modificar el usar una distribución uniforme o normal (por ejemplo, usando el valor `xavier-uniform` para cambiar el comportamiento anterior).

## 2b) Descenso de gradiente con momentum

En esta parte implementarás el descenso de gradiente con momentum lo que puede mejorar considerablemente el tiempo de entrenamiento de tu red. 
En clases/video vimos dos formulaciones, una considerando el promedio exponencial móvil de los gradientes pasados, y otra como una interpretación física. En esta parte implementarás la segunda. Recuerda que en este caso la idea es incrementar una variable de *velocidad* en la dirección contraria del gradiente (que haría las veces de *aceleración*) y usarla para actualizar los parámetros en cada paso del descenso estocástico de gradiente. Recuerda que adicionalmente un (hyper)parámetro $$\mu$$ de *fricción* se utiliza para evitar la oscilación en direcciones de gradiente muy pronunciadas. En particular, para cada conjunto de parámetros $\theta$, y para cada paso del descenso de gradiente se realiza el siguiente cálculo:

$$\begin{eqnarray*}
V_{\partial \theta} & := & \mu V_{\partial \theta} - \lambda \frac{\partial \mathcal{L}}{\partial \theta} \\
\theta & := & \theta + V_{\partial \theta}
\end{eqnarray*}$$


en donde $$V_{\partial \theta}$$ es un tensor de las mismas dimensiones que los parámetros $$\theta$$ y que se inicializa como $$0$$ antes de empezar el entrenamiento.

Modifica la implementación de `SGD` que ya tenías anteriormente para considerar la fórmula de momentum descrita. Para esto agrega un nuevo argumento opcional `momentum` correspondiente al valor $$\mu$$ de la ecuación de arriba, con un valor por defecto de 0.

```python
# Tu código debiera continuar así

class SGD():
  def __init__(self, parameters, lr, beta=0, momentum=0):
    # lo que sea necesario inicializar
    
    pass
  
  def step():
    # actualiza acá los parámetros a partir de los gradientes
    # y considerando el valor de momentum que acabámos de agregar
    pass
```

## 2c) RMSProp

En esta parte implementarás el algoritmo de RMSProp (Root Mean Square Propagation) que mantiene un promedio móvil exponencial de los cuadrados de los gradientes calculados hasta el momento y modifica las tasas de aprendizaje para cada parámetro dependiendo de ese promedio. En específico, RMSProp usa la siguiente regla de actualización para cada conjunto de parámetros $$\theta$$:

\begin{eqnarray*}
S_{\partial \theta} & := & \beta S_{\partial \theta} + (1-\beta)\left(\frac{\partial \mathcal{L}}{\partial \theta}*\frac{\partial \mathcal{L}}{\partial \theta}\right) \\
\theta & := & \theta - \lambda\frac{1}{\sqrt{S_{\partial \theta}}}*\frac{\partial \mathcal{L}}{\partial \theta}
\end{eqnarray*}

donde $$S_{\partial \theta}$$ es un tensor de las mismas dimensiones de $$\theta$$ y se inicializa como $$0$$ antes de emepzar el entrenamiento.
La operación $$*$$ representa a una multiplicación punto a punto.

Implementa una nueva clase `RMSProp` que implemente este optimizador. El inicializador debiera tener dos parámetros: `lr` para la tasa de aprendizaje ($$\lambda$$ en las fórmulas de arriba) y `beta` para el multiplicador del promedio. Asinga los valores por defecto `lr=0.001` y `beta=0.9`. Agrega un parámetro adicional `epsilon` con valor por defecto $$10^{-8}$$, y úsalo para evitar la división por $$0$$ sumándolo al denominador en la fórmula con la raiz cuadrada.

```python
# Tu código acá

class RMSProp():
  def __init__(self, parameters, lr=0.001, beta=0.9, epsilon=1e-8):
    # en este caso debes inicializar la variable que acumula
    # el promedio exponencial de los cuadrados
    pass
  
  def step():
    # actualiza acá los parámetros a partir de los gradientes
    # y la corrección según S
    pass
```

## 2d) Adam

En esta parte implementarás el algoritmo Adam (Adaptive Moments). Adam calcula un promedio exponencial móvil de los valores previos del gradiente y de los cuadrados del gradiente. El promedio de los gradientes lo usa para el paso del descenso del gradiente (como el SGD con momentum) y el promedio de los cuadrados de los gradientes para modificar la tasa de aprendizaje para cada parámetro por separado. Además Adam utiliza una corrección del sesgo inicial para considerar el hecho de que antes del entrenamiento todos los valores del gradiente (y el cuadrado) comienzan como 0. En específico, las actualizaciones de los parámetros según Adam están dadas por:

$$\begin{eqnarray*}
P_{\partial \theta} & := & \beta_1P_{\partial \theta} + (1-\beta_1)\frac{\partial \mathcal{L}}{\partial \theta} \\
S_{\partial \theta} & := & \beta_2 S_{\partial \theta} + (1-\beta_2)\left(\frac{\partial \mathcal{L}}{\partial \theta}*\frac{\partial \mathcal{L}}{\partial \theta}\right) \\
\overline{P_{\partial \theta}} & := & \frac{P_{\partial \theta}}{1-{\beta_1}^n} \\
\overline{S_{\partial \theta}} & := & \frac{S_{\partial \theta}}{1-{\beta_2}^n} \\
\theta & := & \theta - \lambda\frac{1}{\sqrt{\overline{S_{\partial \theta}}}}*\overline{P_{\partial \theta}}
\end{eqnarray*}$$

donde $$P_{\partial \theta}$$ y $$S_{\partial \theta}$$ son tensores de las mismas dimensiones que $$\theta$$ y se inicializan como $$0$$ antes de comenzar el entrenamiento. Adicionalmente $$n$$ indica el paso de la iteración de descenso de gradiente, lo que se usa para corregir el sesgo inicial cuando se está computando $$\overline{P_{\partial \theta}}$$ y $$\overline{S_{\partial \theta}}$$ y la operación $$*$$ representa a una multiplicación punto a punto.

Implementa una nueva clase `Adam` que implemente este optimizador. El inicializador debiera los parámetros `lr`, `beta1` y `beta2`, con valores por defecto 0.001, 0.9, y 0.999, respectivamente. Agrega un parámetro adicional `epsilon` con valor por defecto $$10^{-8}$$, y úsalo para evitar la división por $$0$$ sumándolo al denominador en la fórmula de arriba.

## 2e) Entrenamiento en MNIST 

Usa tu red neuronal para entrenar con los datos de MNIST y compara cómo cambian las curvas de entrenamiento dependiendo de factores como la inicialización y los algoritmos que utilices. Presenta al menos dos gráficos en donde compares. Por ejemplo, puedes presentar uno que para la misma estrategia de inicialización, los tres algoritmos de optimización para varias épocas y cómo evoluciona la pérdida y el acierto. En cada caso comenta que conclusiones puedes sacar. Algunos ejemplos de preguntas que podrías tratar de responder son:
* ¿cómo afecta el algoritmo de optimización al tiempo de convergencia de la red para los datos de entrenamiento?
* ¿cómo afecta el algoritmo de optimización en el acierto alcanzado por la red en los datos de prueba?
* Si haces la parte opcional de Batch Normalization, puedes también preguntarte cosas como si aplicar, o no, BN afecta a todos los algoritmos de optimización por igual.

# Parte 3 (Opcional): Batch Normalization

En esta parte (opcional) programarás la técnica de Batch Normalization (BN). La idea de BN es agregar una nueva capa entre dos capas de la red, digamos las capas $$i$$ e $$i+1$$. Esta nueva capa normaliza los valores de salida de la capa $$i$$ antes de pasarlas como inputs a la capa $$i+1$$. 
Al mismo tiempo, cada capa de BN agrega dos conjuntos de parámetros entrenables que le permiten a la red deshacer la aplicación de la normalización si es que esto ayudara al aprendizaje. 
Más formalmente, supongamos que a una capa BN le pasamos como input la salida $$h^{(i)}$$ de una capa escondida de nuestra red de dimensiones $$(B,d)$$, donde $$B$$ representa a la dimensión del batch de ejemplos. Entonces BN calcula el siguiente valor:
<br>

\begin{eqnarray*}
\tilde{h}^{(i)} := \gamma^{(i)}*\bar{h}^{(i)} + \beta^{(i)}
\end{eqnarray*}
<br>
donde $$\gamma^{(i)}$$ y $$\beta^{(i)}$$ son tensores de parámetros entrenables de dimensión $$(d)$$ y $$\bar{h}^{(i)}$$ es la normalización de $$h^{(i)}$$ respecto a la dimensión del batch. O sea, $$\bar{h}^{(i)}$$ se calcula como:
<br>


$$\bar{h}^{(i)} := \frac{h^{(i)}-\mu^{(i)}}{\sqrt{v^{(i)}}}$$
<br>
Donde $$\mu^{(i)}$$ es un vector de dimensión $$(d)$$ con la media de los valores en $$h^{(i)}$$ (con respecto a la dimensión del batch), y $$v^{(i)}$$ es un vector también de dimensión $$(d)$$ con la varianza de los valores en $$h^{(i)}$$ (con respecto a la dimensión del batch). Nota que las operaciones que generan $$\bar{h}^{(i)}$$ usan broadcasting.

Luego de la capa BN lo que se pasa a la siguiente capa de la red es $$\tilde{h}^{(i)}$$ en vez de $$h^{(i)}$$. O sea, el resultado de la siguiente capa se calcula como
<br>

$$h^{(i+1)} := f^{(i+1)}(\tilde{h}^{(i)}W^{(i+1)}+b^{(i+1)})$$
<br>

Nota que durante el backpropagation deberás considerar la nueva capa. En particular, el gradiente con respecto a $$h^{(i)}$$ ahora pasará por $$\tilde{h}^{(i)}$$ por lo que deberás calcular 
<br>

$$\frac{\partial \mathcal{L}}{\partial h^{(i)}} = \frac{\partial \mathcal{L}}{\partial \tilde{h}^{(i)}} \cdot \frac{\partial \tilde{h}^{(i)}}{\partial {h}^{(i)}}$$
<br>
También deberás calcular los gradientes de $$\mathcal L$$ con respecto a $$\gamma^{(i)}$$ y a $$\beta^{(i)}$$ para poder actualizar esos parámetros. 

Otro aspecto importante de Batch Normalization es que en tiempo de test si le pasamos un único $$x$$ a la red para predecir (no un batch), no tendremos suficientes datos para caluclar $$\mu^{(i)}$$ ni $$v^{(i)}$$. Para esto se mantiene un promedio de los valores de $$\mu^{(i)}$$ y $$v^{(i)}$$ computados durante el entrenamiento (en general un promedio exponencial móvil) y se usan estos valores en tiempo de test.

Implementa BN en tu red neuronal. 
Para esto agrega una nueva lista opcional `bn` de valores booleanos a tu red, de manera tal que el valor correspondiente sea `True` si es que si quiere implementar BN después de la capa correspondiente. Por ejemplo, el siguiente código

```python
red = FFNN(300, [50,30], [relu,sig], 10, bn=[True,False])
```
crea una red de dos capas escondidas, en donde se aplicará BN después de la primera capa escondida (y no después de la segunda capa escondida).
Ten en cuenta que la función `forward` de tu red debe comportarse de manera distinta mientras entrenas y mientras predices. Para esto debes usar la opción `predict` que ya le agregaste al `forward` en la Parte 1 de esta tarea (regularización).

```python
# Tu código debiera continuar como sigue

class FFNN():
  def __init__(self, F, l_h, l_a, C, keep_prob=None, bn=None):
    # debes crear los parámetros necesarios para las capas de
    # batch normalizacion
    pass
  
  def forward(x, predict=False):
    # debes modificar esta función para considerar las capas para las que se
    # usará batch normalization
    # también debes preocuparte de guardar los datos estadísticos que se
    # usaran en tiempo de test (predict=True)
    pass
  
  def backward(x,y,y_pred):
    # computar acá todos los gradientes considerando las capas de 
    # batch normalization
    # no olvides considerar los nuevos parámetros entrenables.
    pass
  ```